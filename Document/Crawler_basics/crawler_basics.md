爬虫基础理论

Date:2017/05/06


## 目录
- [爬虫的目标](#爬虫的目标)
- [内容来源详解](#内容来源详解)
- [了解网络请求](#了解网络请求)
- [一些常见的限制方式](#一些常见的限制方式)
- [解决问题的思路](#解决问题的思路)
- [效率](#效率)

# 爬虫的目标

	一般是抓取,网站获取者app的内容
	内容一般分为

		1.结构化数据(
			a.JSON 数据
			)

		2.非结构化数据(
			a.html 文本
			)
			html 的提取方式(
				1.CSS 选择器
				2.XPATH 元素路径选择方法
				3.RE 正则表达式
				4.字符串切割的方式
				)
# 内容来源详解

	1.网页内容 
		网页内容一般是指网页上看到的内容,但是这是个过程,
		并不是单个网页代码里面直接包含内容这么简单,(
			有这些情况:
			1.静态网页内容
			2.javaScript代码加载的内容 (
					这种情况是由于虽然网页显示时，内容在HTML标签里面，但是其实是由于执行js代码加到标签里面的，所以这个时候内容在js代码里面的，而js的执行是在浏览器端的操作，所以用程序去请求网页地址的时候，得到的response是网页代码和js的代码，所以自己在浏览器端能看到内容，解析时由于js未执行，肯定找到指定HTML标签下内容肯定为空，这个时候的处理办法，一般来讲主要是要找到包含内容的js代码串，然后通过正则表达式获得相应的内容，而不是解析HTML标签。)
			3.Ajax异步请求
				这种情况是现在很常见的,通常返回json内容
		)

	2.app应用
		那么App的数据该如何抓取呢？通用的方法就是抓包,Windows推荐Fiddler2。
		抓到的包其实也是http请求,有了链接就可以直接跟网页请求一样了

# 了解网络请求

	1.URL

	2.请求方法(POST,GET)

	3.request headers (请求头部)

	4.response body (然后内容)

	5.response headers (返回的头部)(
		一个是返回包的内容是空的，但是在返回包的headers的字段里面有个Location，这个Location字段就是告诉浏览器重定向，所以有时候代码没有自动跟踪，自然就没有内容了；
		另外一个就是很多人会头疼的Cookie问题，简单说就是浏览器为什么知道你的请求合法的，例如已登录等等，其实就是可能你之前某个请求的返回包的headers里面有个字段叫Set-Cookie，Cookie存在本地，一旦设置后，除非过期，一般都会自动加在请求字段上，所以Set-Cookie里面的内容就会告诉浏览器存多久，存的是什么内容，在哪个路径下有用，Cookie都是在指定域下，一般都不跨域，域就是你请求的链接host。)
	所以分析请求时，一定要注意前四个，在模拟时保持一致，同时观察第五个返回时是不是有限制或者有重定向。

# 一些常见的限制方式

	1.Basic Auth(
		一般会有用户授权的限制，会在headers的Autheration字段里要求加入)

	2.Referer(
		通常是在访问链接时，必须要带上Referer字段，服务器会进行验证，例如抓取京东的评论)

	3.User-Agent(
		会要求真是的设备，如果不加会用编程语言包里自有User-Agent，可以被辨别出来)

	4.Cookie(
		一般在用户登录或者某些操作后，服务端会在返回包中包含Cookie信息要求浏览器设置Cookie，
		没有Cookie会很容易被辨别出来是伪造请求；
		也有本地通过JS，根据服务端返回的某个信息进行处理生成的加密信息，设置在Cookie里面；)

	5.Gzip(
		请求headers里面带了gzip，返回有时候会是gzip压缩，需要解压)

	6.JavaScript加密操作(
		一般都是在请求的数据包内容里面会包含一些被javascript进行加密限制的信息，例如新浪微博会进行SHA1和RSA加密，之前是两次SHA1加密，然后发送的密码和用户名都会被加密)

	7.其他字段(
		因为http的headers可以自定义地段，所以第三方可能会加入了一些自定义的字段名称或者字段值，这也是需要注意的)

# 解决问题的思路

	1).网页

	1.输入网址后，先不要回车确认，右键选择审查元素，然后点击网络，记得要勾上preserve log选项，因为如果出现上面提到过的重定向跳转，之前的请求全部都会被清掉，影响分析，尤其是重定向时还加上了Cookie

	2.接下来观察网络请求列表，资源文件，例如css，图片基本都可以忽略，第一个请求肯定就是该链接的内容本身，所以查看源码，确认页面上需要抓取的内容是不是在HTML标签里面，很简单的方法，找到自己要找的内容，看到父节点，然后再看源代码里面该父节点里面有没有内容，如果没有，那么一定是异步请求，如果是非异步请求，直接抓该链接就可以了(
		分析异步请求，按照网络列表，略过资源文件，然后点击各个请求，观察是否在返回时包含想要的内容，有几个方法
		a.内容比较有特点，例如人的属性信息，物品的价格，或者微博列表等内容，直接观察可以判断是不是该异步请求
		b.知道异步加载的内容节点或者父节点的class或者id的名称，找到js代码，阅读代码得到异步请求
		c.确认异步请求之后，就是要分析异步请求了，简单的，直接请求异步请求，能得到数据，但是有时候异步请求会有限制，所以现在分析限制从何而来
		)

	3.针对分析对请求的限制，思路是逆序方法(

		a.先找到最后一个得到内容的请求，然后观察headers，先看post数据或者url的某个参数是不是都是已知数据，或者有意义数据，如果发现不确定的先带上，只是更改某个关键字段，例如page，count看结果是不是会正常，如果不正常，比如多了个token，或者某个字段明显被加密，例如用户名密码，那么接下来就要看JS的代码，看到底是哪个函数进行了加密，一般会是原生JS代码加密，那么看到代码，直接加密就行，如果是类似RSA加密，那么就要看公钥是从何而来，如果是请求得到的，那么就要往上分析请求，另外如果是发现请求headers里面有陌生字段，或者有Cookie也要往上看请求，Cookie在哪一步设置的
		b.接下来找到刚刚那个请求未知来源的信息，例如Cookie或者某个加密需要的公钥等等，看看上面某个请求是不是已经包含，依次类推)

	#2).app 

	然后是App类的，使用的工具是Charles，手机和电脑在一个局域网内，先用Charles配置好端口，然后手机设置代理，ip为电脑的ip，端口为设置的端口，然后如果手机上请求网络内容时，Charles会显示相应地请求，那么就ok了，分析的大体逻辑基本一致，限制会相对少很多，但是也有几种情况需要

	1.加密，App有时候也有一些加密的字段，这个时候，一般来讲都会进行反编译进行分析，找到对应的代码片段，逆推出加密方法

	2.gzip压缩或者base64编码，base64编码的辨别度较高，有时候数据被gzip压缩了，不过Charles都是有自动解密的

	3.https证书，有的https请求会验证证书，Charles提供了证书，可以在官网找到，手机访问，然后信任添加就可以

# 效率
